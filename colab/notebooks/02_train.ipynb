{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "02_train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0"
      },
      "source": [
        "# 02_train \u2014 Fine-tune (LoRA/QLoRA) for Preference Extraction\n",
        "\n",
        "Goal: train an instruction-tuned LLM to convert Turkish free-text laptop requests into Laprop `preferences` JSON.\n",
        "\n",
        "This notebook:\n",
        "- loads the JSONL dataset generated by `01_dataset.ipynb`\n",
        "- fine-tunes an instruct model with **4-bit QLoRA** (works on Colab T4)\n",
        "- saves the LoRA adapter to Google Drive (`colab/artifacts/...`)\n",
        "- runs a small generation sanity check on the test set\n",
        "\n",
        "You must enable GPU: `Runtime -> Change runtime type -> GPU`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 0) Drive mount + paths (Colab) ---\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# TODO: adjust to your Drive project folder\n",
        "DRIVE_PROJECT_DIR = Path(\"/content/drive/MyDrive/laprop\")\n",
        "COLAB_DIR = DRIVE_PROJECT_DIR / \"colab\"\n",
        "\n",
        "DATASET_DIR = COLAB_DIR / \"data\" / \"prefs_dataset_v1\"\n",
        "ARTIFACTS_DIR = COLAB_DIR / \"artifacts\" / \"prefs_extractor\"\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"DATASET_DIR:\", DATASET_DIR)\n",
        "print(\"ARTIFACTS_DIR:\", ARTIFACTS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 1) Clone/pull the repo into the Colab runtime ---\n",
        "REPO_DIR = Path(\"/content/laprop-recommender\")\n",
        "REPO_URL = \"https://github.com/ahmedberatAI/laprop-recommender.git\"\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    !git clone --depth 1 {REPO_URL} /content/laprop-recommender\n",
        "else:\n",
        "    !git -C /content/laprop-recommender pull\n",
        "\n",
        "%cd /content/laprop-recommender\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 2) Install dependencies ---\n",
        "%pip install -q -r /content/laprop-recommender/colab/requirements_colab.txt\n",
        "%pip install -q -e /content/laprop-recommender\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 3) GPU sanity check ---\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device:\", torch.cuda.get_device_name(0))\n",
        "    cap = torch.cuda.get_device_capability(0)\n",
        "    print(\"capability:\", cap)\n",
        "else:\n",
        "    cap = (0, 0)\n",
        "\n",
        "# bf16 is typically supported on Ampere+ (A100/L4 etc). T4 is fp16 only.\n",
        "USE_BF16 = bool(torch.cuda.is_available() and cap[0] >= 8)\n",
        "print(\"USE_BF16:\", USE_BF16)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 4) Imports ---\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    set_seed,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 5) Training config (edit these) ---\n",
        "set_seed(42)\n",
        "\n",
        "# A safe default that fits Colab T4 with QLoRA.\n",
        "# You can switch to a larger model if you have more VRAM.\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "# LoRA hyperparams\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Training hyperparams\n",
        "NUM_EPOCHS = 2\n",
        "LEARNING_RATE = 2e-4\n",
        "BATCH_SIZE = 8\n",
        "GRAD_ACCUM = 4\n",
        "WARMUP_RATIO = 0.03\n",
        "\n",
        "RUN_ID = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "MODEL_TAG = MODEL_NAME.split(\"/\")[-1].lower().replace(\".\", \"_\")\n",
        "OUT_DIR = ARTIFACTS_DIR / f\"{MODEL_TAG}_{RUN_ID}\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"OUT_DIR:\", OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 6) Load dataset (from Drive) ---\n",
        "data_files = {\n",
        "    \"train\": str(DATASET_DIR / \"train.jsonl\"),\n",
        "    \"validation\": str(DATASET_DIR / \"val.jsonl\"),\n",
        "    \"test\": str(DATASET_DIR / \"test.jsonl\"),\n",
        "}\n",
        "ds = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "print(ds)\n",
        "print(\"columns:\", ds[\"train\"].column_names)\n",
        "print(\"example:\")\n",
        "ds[\"train\"][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 7) Load tokenizer + base model (4-bit) ---\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "if tokenizer.eos_token is None:\n",
        "    # Most models define EOS; this is a fallback.\n",
        "    tokenizer.eos_token = \"</s>\"\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
        "print(\"eos_token_id:\", tokenizer.eos_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 8) Build supervised fine-tuning examples with prompt-masking ---\n",
        "# We concatenate:  prompt + completion + EOS\n",
        "# and mask labels for the prompt part so the model is trained only on the JSON completion.\n",
        "\n",
        "\n",
        "def find_target_modules(m) -> List[str]:\n",
        "    \"\"\"Auto-detect common projection module names for LoRA.\"\"\"\n",
        "    preferred = [\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ]\n",
        "    present = set()\n",
        "    for name, _ in m.named_modules():\n",
        "        leaf = name.split(\".\")[-1]\n",
        "        if leaf in preferred:\n",
        "            present.add(leaf)\n",
        "    if present:\n",
        "        return sorted(present)\n",
        "\n",
        "    # fallback for other architectures\n",
        "    fallback = [\"query_key_value\", \"dense\", \"fc1\", \"fc2\"]\n",
        "    present = set()\n",
        "    for name, _ in m.named_modules():\n",
        "        leaf = name.split(\".\")[-1]\n",
        "        if leaf in fallback:\n",
        "            present.add(leaf)\n",
        "    return sorted(present)\n",
        "\n",
        "\n",
        "TARGET_MODULES = find_target_modules(model)\n",
        "print(\"TARGET_MODULES:\", TARGET_MODULES)\n",
        "\n",
        "\n",
        "def encode_example(ex: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    prompt = ex[\"prompt\"]\n",
        "    completion = ex[\"completion\"]\n",
        "\n",
        "    prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
        "    comp_ids = tokenizer(completion, add_special_tokens=False)[\"input_ids\"]\n",
        "    comp_ids = comp_ids + [tokenizer.eos_token_id]\n",
        "\n",
        "    # keep completion; truncate prompt first if needed\n",
        "    if len(prompt_ids) + len(comp_ids) > MAX_LEN:\n",
        "        keep = MAX_LEN - len(comp_ids)\n",
        "        if keep <= 0:\n",
        "            prompt_ids = []\n",
        "            comp_ids = comp_ids[:MAX_LEN]\n",
        "        else:\n",
        "            prompt_ids = prompt_ids[-keep:]\n",
        "\n",
        "    input_ids = prompt_ids + comp_ids\n",
        "    labels = ([-100] * len(prompt_ids)) + comp_ids\n",
        "    attn = [1] * len(input_ids)\n",
        "\n",
        "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attn}\n",
        "\n",
        "\n",
        "tokenized = ds.map(\n",
        "    encode_example,\n",
        "    remove_columns=ds[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "print(tokenized)\n",
        "print(\"tokenized example lens:\", len(tokenized[\"train\"][0][\"input_ids\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 9) Data collator (pads input_ids/labels) ---\n",
        "import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM:\n",
        "    tokenizer: Any\n",
        "    pad_to_multiple_of: int = 8\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
        "        if self.pad_to_multiple_of:\n",
        "            m = self.pad_to_multiple_of\n",
        "            max_len = ((max_len + m - 1) // m) * m\n",
        "\n",
        "        pad_id = int(self.tokenizer.pad_token_id)\n",
        "        batch_input_ids = []\n",
        "        batch_attention = []\n",
        "        batch_labels = []\n",
        "        for f in features:\n",
        "            l = len(f[\"input_ids\"])\n",
        "            pad = max_len - l\n",
        "            batch_input_ids.append(f[\"input_ids\"] + [pad_id] * pad)\n",
        "            batch_attention.append(f[\"attention_mask\"] + [0] * pad)\n",
        "            batch_labels.append(f[\"labels\"] + [-100] * pad)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(batch_attention, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(batch_labels, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "collator = DataCollatorForCausalLM(tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c10"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 10) Attach LoRA adapter (QLoRA) ---\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c11"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 11) Train ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=str(OUT_DIR),\n",
        "    num_train_epochs=float(NUM_EPOCHS),\n",
        "    learning_rate=float(LEARNING_RATE),\n",
        "    warmup_ratio=float(WARMUP_RATIO),\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    per_device_train_batch_size=int(BATCH_SIZE),\n",
        "    per_device_eval_batch_size=int(BATCH_SIZE),\n",
        "    gradient_accumulation_steps=int(GRAD_ACCUM),\n",
        "    logging_steps=20,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    fp16=not USE_BF16,\n",
        "    bf16=USE_BF16,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    data_collator=collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c12"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 12) Save adapter to Drive ---\n",
        "adapter_dir = OUT_DIR / \"adapter\"\n",
        "adapter_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "trainer.model.save_pretrained(adapter_dir)\n",
        "tokenizer.save_pretrained(adapter_dir)\n",
        "\n",
        "meta = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"max_len\": MAX_LEN,\n",
        "    \"lora\": {\n",
        "        \"r\": LORA_R,\n",
        "        \"alpha\": LORA_ALPHA,\n",
        "        \"dropout\": LORA_DROPOUT,\n",
        "        \"target_modules\": TARGET_MODULES,\n",
        "    },\n",
        "    \"train\": {\n",
        "        \"epochs\": NUM_EPOCHS,\n",
        "        \"lr\": LEARNING_RATE,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"grad_accum\": GRAD_ACCUM,\n",
        "        \"warmup_ratio\": WARMUP_RATIO,\n",
        "    },\n",
        "}\n",
        "\n",
        "(adapter_dir / \"meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "print(\"saved adapter:\", adapter_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c13"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# --- 13) Quick sanity check on test set (small sample) ---\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def generate_completion(prompt: str, max_new_tokens: int = 180) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=int(max_new_tokens),\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "            top_p=1.0,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    gen = out[0][inputs[\"input_ids\"].shape[1] :]\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "def extract_json_obj(text: str) -> Any:\n",
        "    s = (text or \"\").strip()\n",
        "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
        "    if not m:\n",
        "        return None\n",
        "    cand = m.group(0)\n",
        "    try:\n",
        "        return json.loads(cand)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "sample_n = 30\n",
        "idxs = list(range(len(ds[\"test\"])))\n",
        "random.shuffle(idxs)\n",
        "idxs = idxs[:sample_n]\n",
        "\n",
        "ok_json = 0\n",
        "ok_usage = 0\n",
        "\n",
        "for j, i in enumerate(idxs):\n",
        "    ex = ds[\"test\"][i]\n",
        "    pred_text = generate_completion(ex[\"prompt\"]) \n",
        "    pred_obj = extract_json_obj(pred_text)\n",
        "    gold = ex.get(\"target_prefs\") or {}\n",
        "\n",
        "    if pred_obj is not None:\n",
        "        ok_json += 1\n",
        "        if pred_obj.get(\"usage_key\") == gold.get(\"usage_key\"):\n",
        "            ok_usage += 1\n",
        "\n",
        "    if j < 3:\n",
        "        print(\"---\")\n",
        "        print(\"input:\", ex[\"input_text\"])\n",
        "        print(\"gold:\", gold)\n",
        "        print(\"pred_raw:\", pred_text)\n",
        "        print(\"pred_obj:\", pred_obj)\n",
        "\n",
        "print(\"\\nmetrics (sample):\")\n",
        "print(f\"- json_parse_rate: {ok_json}/{sample_n} = {ok_json/sample_n:.3f}\")\n",
        "print(f\"- usage_key_acc  : {ok_usage}/{sample_n} = {ok_usage/sample_n:.3f}\")\n"
      ]
    }
  ]
}

