{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "03_eval.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03_eval ? Full Test Evaluation (LoRA Adapter)\n",
        "\n",
        "Goal: evaluate the trained preference-extractor adapter on the **full test split**.\n",
        "\n",
        "This notebook:\n",
        "- loads `test.jsonl` from Drive\n",
        "- loads base model + LoRA adapter\n",
        "- runs generation for all test rows\n",
        "- computes JSON validity + key-level metrics\n",
        "- computes usage confusion matrix\n",
        "- writes evaluation artifacts to Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 0) Drive mount + paths ---\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "DRIVE_PROJECT_DIR = Path(\"/content/drive/MyDrive/laprop\")\n",
        "COLAB_DIR = DRIVE_PROJECT_DIR / \"colab\"\n",
        "\n",
        "DATASET_DIR = COLAB_DIR / \"data\" / \"prefs_dataset_v1\"\n",
        "ARTIFACTS_ROOT = COLAB_DIR / \"artifacts\" / \"prefs_extractor\"\n",
        "EVAL_ROOT = COLAB_DIR / \"artifacts\" / \"eval\"\n",
        "EVAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"DATASET_DIR:\", DATASET_DIR)\n",
        "print(\"ARTIFACTS_ROOT:\", ARTIFACTS_ROOT)\n",
        "print(\"EVAL_ROOT:\", EVAL_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1) Clone/pull repo and install deps ---\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_DIR = Path(\"/content/laprop-recommender\")\n",
        "REPO_URL = \"https://github.com/ahmedberatAI/laprop-recommender.git\"\n",
        "\n",
        "if not REPO_DIR.exists():\n",
        "    !git clone --depth 1 {REPO_URL} /content/laprop-recommender\n",
        "else:\n",
        "    !git -C /content/laprop-recommender pull\n",
        "\n",
        "%cd /content/laprop-recommender\n",
        "\n",
        "%pip install -q -r /content/laprop-recommender/colab/requirements_colab.txt\n",
        "%pip install -q -e /content/laprop-recommender\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2) Imports + GPU sanity ---\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import PeftModel\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device:\", torch.cuda.get_device_name(0))\n",
        "    cap = torch.cuda.get_device_capability(0)\n",
        "    print(\"capability:\", cap)\n",
        "else:\n",
        "    cap = (0, 0)\n",
        "\n",
        "USE_BF16 = bool(torch.cuda.is_available() and cap[0] >= 8)\n",
        "print(\"USE_BF16:\", USE_BF16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3) Select adapter dir (auto: latest) ---\n",
        "# Option A: set a specific adapter path manually\n",
        "ADAPTER_DIR_STR = \"\"  # e.g. \"/content/drive/MyDrive/laprop/colab/artifacts/prefs_extractor/.../adapter\"\n",
        "\n",
        "if ADAPTER_DIR_STR.strip():\n",
        "    ADAPTER_DIR = Path(ADAPTER_DIR_STR)\n",
        "else:\n",
        "    cands = sorted(ARTIFACTS_ROOT.glob(\"*/adapter\"))\n",
        "    if not cands:\n",
        "        raise FileNotFoundError(f\"No adapter found under: {ARTIFACTS_ROOT}\")\n",
        "    ADAPTER_DIR = cands[-1]\n",
        "\n",
        "meta_path = ADAPTER_DIR / \"meta.json\"\n",
        "if not meta_path.exists():\n",
        "    raise FileNotFoundError(f\"meta.json not found: {meta_path}\")\n",
        "\n",
        "meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))\n",
        "MODEL_NAME = meta.get(\"model_name\", \"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "MAX_LEN = int(meta.get(\"max_len\", 512))\n",
        "\n",
        "run_id = ADAPTER_DIR.parent.name\n",
        "EVAL_DIR = EVAL_ROOT / f\"{run_id}_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
        "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"ADAPTER_DIR:\", ADAPTER_DIR)\n",
        "print(\"MODEL_NAME:\", MODEL_NAME)\n",
        "print(\"MAX_LEN:\", MAX_LEN)\n",
        "print(\"EVAL_DIR:\", EVAL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4) Load test dataset ---\n",
        "data_files = {\"test\": str(DATASET_DIR / \"test.jsonl\")}\n",
        "ds = load_dataset(\"json\", data_files=data_files)\n",
        "test_ds = ds[\"test\"]\n",
        "\n",
        "print(test_ds)\n",
        "print(\"columns:\", test_ds.column_names)\n",
        "print(\"num_rows:\", len(test_ds))\n",
        "print(\"example:\")\n",
        "test_ds[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5) Load base model + LoRA adapter ---\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.eos_token is None:\n",
        "    tokenizer.eos_token = \"</s>\"\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, str(ADAPTER_DIR))\n",
        "model.eval()\n",
        "\n",
        "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
        "print(\"eos_token_id:\", tokenizer.eos_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6) Inference helpers ---\n",
        "def extract_json_obj(text: str) -> Optional[Dict[str, Any]]:\n",
        "    s = (text or \"\").strip()\n",
        "    if not s:\n",
        "        return None\n",
        "\n",
        "    # direct parse\n",
        "    try:\n",
        "        obj = json.loads(s)\n",
        "        return obj if isinstance(obj, dict) else None\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # fenced block parse\n",
        "    fence = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", s, flags=re.S)\n",
        "    if fence:\n",
        "        cand = fence.group(1)\n",
        "        try:\n",
        "            obj = json.loads(cand)\n",
        "            return obj if isinstance(obj, dict) else None\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # first JSON-looking object\n",
        "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
        "    if m:\n",
        "        cand = m.group(0)\n",
        "        try:\n",
        "            obj = json.loads(cand)\n",
        "            return obj if isinstance(obj, dict) else None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def to_float_or_none(x: Any) -> Optional[float]:\n",
        "    try:\n",
        "        if x is None:\n",
        "            return None\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def normalize_value(v: Any) -> Any:\n",
        "    if isinstance(v, list):\n",
        "        return [normalize_value(x) for x in v]\n",
        "    if isinstance(v, float) and v.is_integer():\n",
        "        return int(v)\n",
        "    return v\n",
        "\n",
        "\n",
        "def generate_completion(prompt: str, max_new_tokens: int = 180) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=int(max_new_tokens),\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    return tokenizer.decode(gen, skip_special_tokens=True).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7) Run full test inference ---\n",
        "rows = []\n",
        "\n",
        "for i in range(len(test_ds)):\n",
        "    ex = test_ds[i]\n",
        "    prompt = ex[\"prompt\"]\n",
        "    pred_raw = generate_completion(prompt)\n",
        "    pred_obj = extract_json_obj(pred_raw)\n",
        "\n",
        "    rows.append({\n",
        "        \"idx\": i,\n",
        "        \"id\": ex.get(\"id\"),\n",
        "        \"scenario_id\": ex.get(\"scenario_id\"),\n",
        "        \"input_text\": ex.get(\"input_text\"),\n",
        "        \"gold\": ex.get(\"target_prefs\") or {},\n",
        "        \"pred_raw\": pred_raw,\n",
        "        \"pred\": pred_obj,\n",
        "        \"json_ok\": pred_obj is not None,\n",
        "    })\n",
        "\n",
        "print(\"inference done:\", len(rows))\n",
        "print(\"json ok:\", sum(r[\"json_ok\"] for r in rows), \"/\", len(rows))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 8) Compute metrics ---\n",
        "KEYS = [\n",
        "    \"usage_key\",\n",
        "    \"min_budget\",\n",
        "    \"max_budget\",\n",
        "    \"dev_mode\",\n",
        "    \"productivity_profile\",\n",
        "    \"screen_max\",\n",
        "    \"design_profiles\",\n",
        "    \"design_gpu_hint\",\n",
        "    \"design_min_ram_hint\",\n",
        "    \"gaming_titles\",\n",
        "    \"min_gpu_score_required\",\n",
        "    \"gaming_min_gpu\",\n",
        "]\n",
        "\n",
        "n = len(rows)\n",
        "json_ok = sum(r[\"json_ok\"] for r in rows)\n",
        "json_parse_rate = json_ok / n if n else 0.0\n",
        "\n",
        "core_keys = [\"usage_key\", \"min_budget\", \"max_budget\"]\n",
        "core_exact = 0\n",
        "\n",
        "usage_gold = []\n",
        "usage_pred = []\n",
        "\n",
        "per_key = {}\n",
        "for k in KEYS:\n",
        "    per_key[k] = {\"support\": 0, \"correct\": 0}\n",
        "\n",
        "min_budget_abs_err = []\n",
        "max_budget_abs_err = []\n",
        "\n",
        "for r in rows:\n",
        "    gold = r[\"gold\"] or {}\n",
        "    pred = r[\"pred\"] or {}\n",
        "\n",
        "    # core exact\n",
        "    ok_core = True\n",
        "    for k in core_keys:\n",
        "        gv = gold.get(k)\n",
        "        pv = pred.get(k)\n",
        "        if isinstance(gv, (int, float)):\n",
        "            if to_float_or_none(pv) is None or float(gv) != float(pv):\n",
        "                ok_core = False\n",
        "                break\n",
        "        else:\n",
        "            if gv != pv:\n",
        "                ok_core = False\n",
        "                break\n",
        "    if ok_core:\n",
        "        core_exact += 1\n",
        "\n",
        "    # usage confusion\n",
        "    ug = gold.get(\"usage_key\")\n",
        "    up = pred.get(\"usage_key\") if pred else None\n",
        "    usage_gold.append(ug)\n",
        "    usage_pred.append(up if up is not None else \"<NONE>\")\n",
        "\n",
        "    # per-key accuracy over non-null gold labels\n",
        "    for k in KEYS:\n",
        "        gv = gold.get(k)\n",
        "        if gv is None:\n",
        "            continue\n",
        "\n",
        "        per_key[k][\"support\"] += 1\n",
        "        pv = pred.get(k) if pred else None\n",
        "\n",
        "        if k in (\"min_budget\", \"max_budget\", \"min_gpu_score_required\", \"gaming_min_gpu\", \"design_min_ram_hint\"):\n",
        "            gvn = to_float_or_none(gv)\n",
        "            pvn = to_float_or_none(pv)\n",
        "            if gvn is not None and pvn is not None and gvn == pvn:\n",
        "                per_key[k][\"correct\"] += 1\n",
        "        elif isinstance(gv, list):\n",
        "            if isinstance(pv, list) and [str(x) for x in pv] == [str(x) for x in gv]:\n",
        "                per_key[k][\"correct\"] += 1\n",
        "        else:\n",
        "            if normalize_value(pv) == normalize_value(gv):\n",
        "                per_key[k][\"correct\"] += 1\n",
        "\n",
        "    # budget MAE\n",
        "    gmin = to_float_or_none(gold.get(\"min_budget\"))\n",
        "    pmin = to_float_or_none(pred.get(\"min_budget\") if pred else None)\n",
        "    if gmin is not None and pmin is not None:\n",
        "        min_budget_abs_err.append(abs(gmin - pmin))\n",
        "\n",
        "    gmax = to_float_or_none(gold.get(\"max_budget\"))\n",
        "    pmax = to_float_or_none(pred.get(\"max_budget\") if pred else None)\n",
        "    if gmax is not None and pmax is not None:\n",
        "        max_budget_abs_err.append(abs(gmax - pmax))\n",
        "\n",
        "per_key_rows = []\n",
        "for k, v in per_key.items():\n",
        "    supp = v[\"support\"]\n",
        "    corr = v[\"correct\"]\n",
        "    acc = (corr / supp) if supp else None\n",
        "    per_key_rows.append({\"key\": k, \"support\": supp, \"correct\": corr, \"accuracy\": acc})\n",
        "\n",
        "per_key_df = pd.DataFrame(per_key_rows).sort_values([\"support\", \"key\"], ascending=[False, True])\n",
        "\n",
        "metrics = {\n",
        "    \"n_test\": n,\n",
        "    \"json_ok\": json_ok,\n",
        "    \"json_parse_rate\": json_parse_rate,\n",
        "    \"core_exact_count\": core_exact,\n",
        "    \"core_exact_rate\": (core_exact / n if n else 0.0),\n",
        "    \"min_budget_mae\": (float(np.mean(min_budget_abs_err)) if min_budget_abs_err else None),\n",
        "    \"max_budget_mae\": (float(np.mean(max_budget_abs_err)) if max_budget_abs_err else None),\n",
        "}\n",
        "\n",
        "print(json.dumps(metrics, indent=2, ensure_ascii=False))\n",
        "print(\"\n",
        "per-key accuracy:\")\n",
        "display(per_key_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 9) Usage confusion matrix + report ---\n",
        "labels = sorted(set(usage_gold) | set(usage_pred))\n",
        "cm = confusion_matrix(usage_gold, usage_pred, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"gold:{x}\" for x in labels], columns=[f\"pred:{x}\" for x in labels])\n",
        "\n",
        "print(\"labels:\", labels)\n",
        "print(\"\n",
        "confusion matrix:\")\n",
        "display(cm_df)\n",
        "\n",
        "print(\"\n",
        "classification report (usage_key):\")\n",
        "print(classification_report(usage_gold, usage_pred, labels=labels, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 10) Save artifacts to Drive ---\n",
        "preds_path = EVAL_DIR / \"predictions.jsonl\"\n",
        "metrics_path = EVAL_DIR / \"metrics.json\"\n",
        "per_key_path = EVAL_DIR / \"per_key_accuracy.csv\"\n",
        "cm_path = EVAL_DIR / \"usage_confusion.csv\"\n",
        "\n",
        "with preds_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for r in rows:\n",
        "        row = {\n",
        "            \"idx\": r[\"idx\"],\n",
        "            \"id\": r[\"id\"],\n",
        "            \"scenario_id\": r[\"scenario_id\"],\n",
        "            \"input_text\": r[\"input_text\"],\n",
        "            \"gold\": r[\"gold\"],\n",
        "            \"pred\": r[\"pred\"],\n",
        "            \"pred_raw\": r[\"pred_raw\"],\n",
        "            \"json_ok\": r[\"json_ok\"],\n",
        "        }\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\n",
        "\")\n",
        "\n",
        "metrics_path.write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "per_key_df.to_csv(per_key_path, index=False)\n",
        "cm_df.to_csv(cm_path)\n",
        "\n",
        "print(\"wrote:\")\n",
        "print(\"-\", preds_path)\n",
        "print(\"-\", metrics_path)\n",
        "print(\"-\", per_key_path)\n",
        "print(\"-\", cm_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 11) Show failure samples ---\n",
        "fail = []\n",
        "for r in rows:\n",
        "    gold = r[\"gold\"] or {}\n",
        "    pred = r[\"pred\"] or {}\n",
        "\n",
        "    # focus mismatch: usage_key, min_budget, max_budget\n",
        "    bad = False\n",
        "    if not r[\"json_ok\"]:\n",
        "        bad = True\n",
        "    else:\n",
        "        for k in [\"usage_key\", \"min_budget\", \"max_budget\"]:\n",
        "            gv = gold.get(k)\n",
        "            pv = pred.get(k)\n",
        "            if isinstance(gv, (int, float)):\n",
        "                if to_float_or_none(pv) is None or float(gv) != float(pv):\n",
        "                    bad = True\n",
        "                    break\n",
        "            else:\n",
        "                if gv != pv:\n",
        "                    bad = True\n",
        "                    break\n",
        "\n",
        "    if bad:\n",
        "        fail.append(r)\n",
        "\n",
        "print(\"fail_count:\", len(fail))\n",
        "for r in fail[:10]:\n",
        "    print(\"---\")\n",
        "    print(\"id:\", r[\"id\"], \"scenario:\", r[\"scenario_id\"])\n",
        "    print(\"input:\", r[\"input_text\"])\n",
        "    print(\"gold:\", r[\"gold\"])\n",
        "    print(\"pred:\", r[\"pred\"])\n",
        "    print(\"pred_raw:\", r[\"pred_raw\"][:300])\n"
      ]
    }
  ]
}
